---
globs:
description: This rule file outlines comprehensive best practices for using pytest in Python projects, covering code organization, testing strategies, performance optimization, security measures, and common pitfalls to avoid.
alwaysApply: false
---

# Testing Best Practices

## Test Infrastructure Preservation

**⚠️ CRITICAL: Preserve Existing Test Infrastructure**

When working with existing test infrastructure:

• **Never replace** comprehensive test setup files (like `conftest.py`)
• **Always add** new fixtures to existing infrastructure
• **Don't add `run_test_` scripts. Use the existing infrastructure to run the tests with `python tests/run_tests.py`
• **Understand dependencies** before making changes
• **Test the impact** of changes on existing tests
• **Preserve database setup** (PostgreSQL containers, connection handling)
• **Maintain existing fixtures** (OHLCV data, strategies, risk parameters)
• **Follow established patterns** for consistency
• **Document new fixtures** with clear docstrings

## Test Organization

• Use pytest for all testing
• Organize tests by component/feature
• Use descriptive test names that explain the scenario
• Group related tests in classes
• Use fixtures for shared test data and setup
• Mark tests with appropriate pytest markers

## Test Categories

• **Unit Tests**: Test individual components in isolation
• **Integration Tests**: Test component interactions
• **End-to-End Tests**: Test complete workflows
• **Performance Tests**: Test timing and resource usage
• **Error Handling Tests**: Test exception scenarios

## Test Data Management

• Use realistic test data that mimics production scenarios
• Generate test data programmatically when possible
• Use fixtures for reusable test data
• Clean up test data after tests complete
• Use separate test databases for integration tests

## Mocking and Stubbing

• Mock external dependencies (APIs, databases, file systems)
• Use dependency injection to make components testable
• Mock time-dependent operations for deterministic tests
• Use realistic mock responses that match API contracts
• Test both success and failure scenarios

## Error Testing

• Test all error conditions and edge cases
• Verify error messages are helpful and accurate
• Test exception handling and recovery
• Test boundary conditions (empty data, null values, etc.)
• Test rate limiting and timeout scenarios

## Performance Testing

• Test with realistic data volumes
• Measure execution time for critical paths
• Test memory usage under load
• Use profiling tools to identify bottlenecks
• Test concurrent access patterns

## Test Documentation

• Write clear test descriptions that explain the business logic
• Document test data requirements and assumptions
• Explain complex test setups in comments
• Document test dependencies and prerequisites
• Keep test documentation up to date

## Continuous Testing

• Run tests automatically on code changes
• Use CI/CD pipelines for automated testing
• Maintain high test coverage for critical components
• Monitor test execution time and optimize slow tests
• Use test reports to track quality metrics
