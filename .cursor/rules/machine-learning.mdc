---
description: machine learning, model training, deep learning
alwaysApply: false
---
## Machine Learning Best Practices & Overfitting Prevention

### üß† Core ML Principles
**Overfitting**: When models memorize training data instead of learning generalizable patterns. Leads to great training performance but poor real-world results.

**Key Lesson**: Our ML Basic Strategy (5 features, 18.34% annual returns) vastly outperformed ML Premium Strategy (14 features, -12.31% annual returns) over 5 years due to better generalization.

### ‚úÖ Model Development Guidelines

#### 1. **Start Simple, Add Complexity Carefully**
```python
# ‚úÖ Good: Start with essential features
features_v1 = ['close', 'volume', 'high', 'low', 'open']

# ‚ùå Bad: Start with everything
features_v1 = ['close', 'volume', 'high', 'low', 'open', 'sentiment_primary', 
               'sentiment_momentum', 'sentiment_volatility', 'sentiment_extreme_positive', 
               'sentiment_extreme_negative', 'sentiment_ma_3', 'sentiment_ma_7', 
               'sentiment_ma_14', 'sentiment_confidence']
```

#### 2. **Feature Engineering Rules**
- **Domain-First**: Use trading knowledge to select meaningful features
- **Correlation Check**: Remove highly correlated features (>0.8)
- **Feature Importance**: Track which features actually contribute to predictions
- **Economic Logic**: Each feature should have a clear economic rationale
- **Less is More**: 5-8 features often outperform 15+ features

#### 3. **Validation Strategy (Critical)**
```python
# ‚úÖ Time-Based Cross-Validation for Trading
train_periods = [
    ('2020-01', '2021-12'), ('2021-07', '2023-06'), 
    ('2022-01', '2024-01'), ('2023-01', '2025-01')
]
test_periods = [
    ('2022-01', '2022-12'), ('2023-07', '2024-06'),
    ('2024-02', '2025-02'), ('2025-02', '2025-12')
]

# ‚ùå Bad: Random train/test split (ignores time series nature)
X_train, X_test = train_test_split(data, test_size=0.2, random_state=42)
```

#### 4. **Performance Monitoring**
- **Training vs Validation Gap**: >10% difference = potential overfitting
- **Multiple Timeframes**: Test on 1-year, 3-year, 5-year periods
- **Market Regime Changes**: Validate across bull/bear/sideways markets
- **Walk-Forward Analysis**: Retrain periodically, test on future data

#### 5. **Model Complexity Guidelines**
```python
# Rule of Thumb: Need ~10x training samples per feature
features = 5  # Need ~50+ training examples minimum
features = 14 # Need ~140+ training examples minimum

# Consider model capacity
if training_samples < features * 10:
    print("‚ö†Ô∏è Potential overfitting risk - reduce features or get more data")
```

### üö® Overfitting Warning Signs

#### Immediate Red Flags:
- **Perfect training accuracy** (>95% on complex data)  
- **Training loss keeps decreasing while validation loss increases**
- **Too many parameters** relative to training data size
- **Model can't explain why** it makes predictions (black box issues)

#### Long-Term Red Flags:
- **Great backtest, poor live performance** (our Premium Strategy case)
- **Performance degrades over time** without market regime changes
- **Strategy works only in specific market conditions**
- **Win rate dramatically different** between training and live periods

### üõ°Ô∏è Overfitting Prevention Techniques

#### 1. **Regularization**
```python
# L1/L2 regularization
model = LinearRegression(alpha=0.01, l1_ratio=0.5)  # Elastic Net

# Dropout for neural networks  
model.add(Dropout(0.2))

# Early stopping
early_stop = EarlyStopping(patience=10, restore_best_weights=True)
```

#### 2. **Feature Selection**
```python
# Recursive feature elimination
selector = RFE(estimator, n_features_to_select=5)

# Statistical significance
p_values = f_regression(X, y)[1]
selected_features = X.columns[p_values < 0.05]

# Economic significance (domain knowledge)
essential_features = ['close', 'volume', 'high', 'low', 'open']
```

#### 3. **Ensemble Methods**
```python
# Combine multiple simple models instead of one complex model
models = [
    RandomForestRegressor(max_depth=3),
    LinearRegression(),
    GradientBoostingRegressor(max_depth=2)
]
```

### üìä Trading-Specific ML Considerations

#### Data Quality & Stationarity:
- **Check for data leakage**: Future information in features
- **Handle missing data**: Forward-fill prices, neutral-fill sentiment
- **Address non-stationarity**: Returns vs prices, differencing
- **Account for regime changes**: Markets evolve, models must adapt

#### Feature Engineering:
```python
# ‚úÖ Good: Stationary features
returns = prices.pct_change()
rolling_volatility = returns.rolling(20).std()
rsi = calculate_rsi(prices, 14)

# ‚ùå Bad: Non-stationary features  
raw_prices = data['close']  # Trends upward, causes issues
cumulative_volume = data['volume'].cumsum()  # Ever-increasing
```

#### Model Selection for Trading:
- **Linear models**: Interpretable, fast, good for mean reversion
- **Tree models**: Handle non-linearities, feature interactions
- **Neural networks**: Complex patterns, but high overfitting risk
- **Ensemble**: Often best balance of accuracy and robustness

### üéØ Development Workflow

#### Phase 1: Explore & Validate
1. **Start with basic technical indicators** (5-8 features max)
2. **Use walk-forward validation** on historical data
3. **Test multiple timeframes** (1D, 4H, 1H)
4. **Check economic intuition** - do results make sense?
#### Phase 2: Iterate & Improve  
1. **Add features one at a time** and measure impact
2. **Remove redundant/low-importance features**
3. **Tune hyperparameters** using validation data only
4. **Document feature rationale** and performance impact

#### Phase 3: Validate & Deploy
1. **Final test on truly unseen data** (never used in development)
2. **Paper trade** before live deployment
3. **Monitor model drift** and retrain schedule
4. **Have fallback strategy** if model fails

### üöÄ Key Takeaways

1. **Simple Models Win Long-Term**: Our 5-feature model beat 14-feature model
2. **Overfitting is Expensive**: -48% vs +132% total returns over 5 years  
3. **Domain Knowledge > Feature Engineering**: Trading intuition beats complex features
4. **Validate Like You Trade**: Time-series splits, multiple periods, regime changes
5. **Monitor Continuously**: Models decay, markets evolve, stay vigilant

### üìù Model Development Checklist

Before deploying any ML trading model:
- [ ] Features have clear economic rationale
- [ ] Training/validation gap < 10%  
- [ ] Tested on multiple time periods (1Y, 3Y, 5Y)
- [ ] Validated across different market regimes
- [ ] Simple baseline model exists for comparison
- [ ] Feature importance documented and sensible
- [ ] Model can be explained to non-technical stakeholders
- [ ] Fallback strategy defined for model failure
- [ ] Retraining schedule established
- [ ] Performance monitoring dashboard ready

**Remember**: In trading, being approximately right consistently beats being precisely wrong. Favor robust simplicity over complex precision.
